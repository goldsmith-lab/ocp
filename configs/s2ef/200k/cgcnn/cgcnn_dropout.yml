includes:
- configs/s2ef/200k/base.yml

model:
  name: cgcnn_dropout
  atom_embedding_size: 128
  fc_feat_size: 128
  num_fc_layers: 3
  num_graph_conv_layers: 2
  cutoff: 6.0
  num_gaussians: 100
  use_pbc: True
  use_dropout: True
  dropout_rate: 0.30


# *** Important note ***
#   The total number of gpus used for this run was 4.
#   If the global batch size (num_gpus * batch_size) is modified
#   the lr_milestones and warmup_steps need to be adjusted accordingly.
optim:
#32 is is a good default to balance training time/training loss in the absence of data.
  batch_size: 32
  eval_batch_size: 32
  # Try 1 worker per CPU core as a good first guess, even when doing GPU training.
  num_workers: 16
  lr_initial: 0.0005
  lr_gamma: 0.1
  lr_milestones: # steps at which lr_initial <- lr_initial * lr_gamma
    - 23437
    - 31250
  warmup_steps: 3125
  warmup_factor: 0.2
  max_epochs: 50
  force_coefficient: 10

#trainer: forces # Use the ForcesTrainer

#dataset:
  # Training data
#- src: data/s2ef/25k/train
#  normalize_labels: True
  # Mean and standard deviation of energies
#  target_mean: -0.7586356401443481
#  target_std: 2.981738567352295
  # Mean and standard deviation of forces
#  grad_target_mean: 0.0
#  grad_target_std: 2.981738567352295
  # Val data (optional)
#- src: data/s2ef/all/val_ood_both/
  # Test data (optional)
#- src: data/s2ef/s2ef_test_lmdbs/test_data/s2ef/all/test_ood_both/
